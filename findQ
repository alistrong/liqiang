1 定位CPU高的原因：
   首先：根据输入top命令后 再输入P， 查看cpu消耗高的pid。 
   然后：根据命令 top -P -a pid 查看这个pid下面消耗CPU最大的线程 tid  (printf '%x\n' tid 得到tid对应的十六进制)
   最后：jstack pid|grep tid(十六进制)            jstack(查看当前所有线程的运行情况和线程当前状态)
   
   jstack 文件：  1   死循环 
                  2   wait()  Thread.State: WAITING
                  3   死锁    deadlock
                  4   等待IO  
2 定位堆溢出问题：
   首先：利用jstat -gcutil pid 查看堆内存使用情况.
   然后：jmap dump:format=b,file =[文件输出路径][pid] 得到dump出来的文件              jmap(监视进程运行中的jvm物理内存的占用情况)
   最后：利用分析工具，如MAT

3 OOM outofmemory
  某个Java服务 出现了OOM，最常见的原因：
  1 内存分配确实过小
  2 某个对象频繁申请，但是没释放，内存泄露导致内存耗尽
  3 某个资源频繁申请，系统资源耗尽，如：不断创建线程，不断发起网络请求
  
  -- jmap -heap pid // 可查看新生代，老年代堆内存的分配大小及使用情况，看是否本身分配过小
  -- 找最耗内存的对象 jmap -histo:live pid |more  会展示 按照内存大小排序的存活对象信息（实例数|所占内存大小|类名）
     如果发现某个类对象占用内存很大，很可能类对象创建太多，且一直未释放
  -- 确认是否资源耗尽
     pstree 
     netstat
     查看进程创建的线程数，网络连接数，如果资源耗尽有可能出现OOM
   
   如果线上系统运行缓慢，导致线上系统不可用，首先要做的是导出jstack和内存信息，重启系统，尽快保证系统可用性
   可能原因：1 某个位置读取数据量大，导致系统内存耗尽，从而导致full gc次数过多，系统缓慢
            2 代码确实有比较耗CPU的操作，导致CPU过高，
   另外有几种情况会导致某个功能缓慢，但不至于导致系统不可用
            1 代码某个位置有阻塞性操作，导致耗时升高，比较随机
            2 某个线程因为某种原因进入WATIING状态，此时该功能整体不可用，但是无法复现
            3 由于锁使用不当，导致死锁，进而导致系统比较缓慢
   上面三种情况一般CPU和内存使用没有问题，但是功能变慢（相对来说是有阻塞性操作）需要通过系统日志一步一步查看
  
  1 Full GC次数过多，主要有如下特征
     - 线上多个线程的CPU都超过了100%，通过jstack命令可以看到主要是垃圾回收线程
     - 通过jstat命令监控GC情况，发现Full GC次数非常多，
     
  通过jstack 看到如下信息： "VM Thread" os_prio=0 tid=0x0007f821736e00 nid = oxa runnable  // VM Thread就是垃圾回收线程
  
  通过jstat查看GC情况，频率很高，证实由于内存溢出导致的系统变慢，内存溢出，如何查看哪些对象呢？dump出内存日志，通过MAT工具查看
  
  2 不定期出现接口耗时情况
  一般这种情况CPU和内存都没有问题，无法通过上述方式解决
  思路：首先找到该接口，通过压测工具不断加大访问力度，如果该接口中某个位置比较耗时，由于我们访问的频率很高，那么大多数的线程最终都将阻塞于该阻塞点
       这样通过多个线程具有相同的堆栈日志，基本可以定位耗时的代码位置。
       
       
 线上问题排查的常用命令
 
 1 了解机器连接数情况
 netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S)print a, S[a]}'
 
 2 备份服务的技巧，打包/opt/web/wfweb 目录，但是又不想打包目录中的logs目录，将打包好的文件存放在/opt/backup目录下
 tar -zcvf /opt/backup/strong.tar.gz \
     -exclude /opt/web/wfweb/logs \
     /opt/web/wfweb
     
 3 查询线程数
    ps -eLf | wc -l
    pstree -p |wc -l
   
 4 磁盘报警，清空最大文件
   首先找到改文件
     find /-type f -name "*log*"|xargs ls -lSh|more
     find / -name '*log*' -size + 1000M -exec du -h {} \
   将文件清空
     echo "">a.log
 
 5  显示server.conf文件，但是屏蔽#开头的注释行
   grep -v "^#" server.conf
 
 6 磁盘IO异常
    查出导致磁盘IO异常高的进程ID
    iotop -o // 查询正在写磁盘操作的所有进程ID信息
    
    如果各项写入指标很低，没有大量的写操作，进行磁盘本身排查，可以查看系统
    dmesg 或者 cat /var/log/message
    
  来一个具体的OOMcase：
  首先收到报警,机器服务全都不可用，首先要重启服务，保证线上服务可用，第一想要dump内存状态，看一些监控指标（发现从几点开始，创建的线程一直在增长）
  差不多问题就找到了，有问题的代码导致线程一直在创建 且创建的线程一直未消亡。
  主要修改的代码是HttpClient的东西，初始化的时候加了一个evictExpiredConnections配置
  整个事件的前因后果
  首先我们先来了解一下http的keep-alive机制
  正常的一个TCP连接需要经过三次握手建立后，发送数据，再经过四次挥手断开连接，每个TCP在server返回response以后断开，如果发起多个http请求多次建立和断开
  TCP，很耗性能，解决方案：即在server返回response不立即断开TCP链接，而是复用这条链接进行下一次的Http请求，则无形省略了TPC创建和断开的开销。
  而Keep-alive（持久链接，链接复用）做的就是复用链接，保证链接持久有效，天下没有免费午餐，keep-alive虽然省去了很多不必要的握手，挥手操作，但是由于
  连接长期保活，如果一直没有http请求的话，连接长时间空闲，会占用资源，有时反而会比复用连接带来更多的性能消耗
  所以一般为keep-alive设置timeout,但是又会引入新的问题
  如果服务端关闭连接，发送FIN包（在设置timeout时间内服务端一直没有收到客户端请求，服务端主动发起带FIN的请求断开连接），在这个FIN包发送但是还没达到
  客户端期间，客户端如果继续复用这个链接发送HTTP请求时，服务端会在四次挥手期间不接受报文而发送RST报文给客户端，客户端收到RST报文就会提示异常 
  
  服务端                客户端
  -
  -
  timeout
  -
  - -------FIN--------->
  < -------send---------
  --------------------->(四次挥手期间不接受报文，发送RST)
  
  针对上线的问题有两种解决方案：
  1 收到异常后，重试，但是注意重试次数，防止雪崩
  2 设置定时线程，定时清理上述的闲置连接，将这个定时时间设置为keepalive timeout时间的一半保证超时前回收
  
  而evictExpiredConnections就是第二种策略，由于代码bug导致每一个请求创建一个HttpClient，每创建每一个HttpClient实例都会创建定时线程
  
  解决问题：HttpClient改成单例，保证服务启动会有一个定时清理线程
  
