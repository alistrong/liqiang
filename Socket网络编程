本地的进程间通讯方式：
1 消息传递（管道，FIFO,消息队列）
2 同步（互斥量，信号量，锁）
3 共享内存
4 远程过程调用
远程的进程间如何通讯呢？首先需要唯一标识这个进程，ip+协议+端口号，即基于socket来通讯，

流量控制：TCP滑动窗口：动态改变窗口大小来调整数据传输。这是一种流量控制方法，当发送窗口和接收窗口的大小都等于1时，就是停止等待协议
拥塞控制：防止过多的数据到网络，导致网络过载，
         慢开始，拥塞控制 ,刚开始数据窗口指数增长，当到达一直阈值后，线性增长，当达到拥塞时，阈值改为原来的一半，wind=1,重新慢开始。。。
         快重传，快恢复：
         
知识背景：
内核是软件和硬件的一个中间层，就是将应用请求传递给硬件，充当底层的驱动，从应用层面来讲，应用程序与硬件没有关系，只与内核有关系

应用程序
   |
   |
   |（系统调用）
内核空间
   |
   |
  硬件
  
再来讲一下IO的流程：
应用程序发起IO请求，请求通过内核系统调用调用底层硬件的IO操作

代码级别的服务端网络请求监听流程：

应用程序：new Socket() 得到fd ，然后该socket绑定一个端口号，然后处在listen状态，通过accept监听客户端请求，（上述的接口都会通过系统调用内核）
正常accept后进入阻塞状态，如果有客户端连接过来，accept处理连接请求，如果有数据过来（阻塞），就recvfrom或者其他方式去读数据

BIO(blocking IO)
通过多线程对应多客户端连接：即主线程accpet阻塞监听网络请求，当有一个客户端请求来时，新起一个线程处理客户端请求
问题：线程太多了（系统资源消耗，线程占内存，系统切换消耗资源），最主要的是阻塞，
因此解决：可以设置为非阻塞，

如果accpet监听请求，没有就继续往下走，recvFrom读取数据，如果没有，也继续往下走
问题：C10k，如果有1w客户端请求连接过来，(f1,..,f10000)，每循环中都有O(n)的系统调用recvFrom,但是10000次可能只有2个有数据，浪费

因此内核帮助我们做，select（）{多路复用}可以监听很多文件描述符，等待1个或多个文件描述符变为可用状态，即不用自己循环判断fd是否可用，即select返回了
可读可写的状态，最终recvfrom调用的都是真实产生读写请求的fd。
问题：1次select()传递n个fd，内核内部主动循环遍历n个fd，

首先要减少fd的传入，即内核里开辟一个空间，每次有新的fd，直接放进去。其次，需要判断可读可写，从主动遍历改为：基于事件驱动，即中断，如果有m个可读可写，
专门放到另外一个区域，即为Epoll模型

new socket()--->5 创建监听连接请求的socket
new epoll_create()---->6 
epoll_ctl(6,add,5,accpet) ----> 将监听请求的socket事件放入到epoll中
epoll_wait(6)--->处理活跃的socket事件

有些软件是使用epoll模型
1 redis
2 nginx
同样使用epoll，redis为啥轮询，nginx为啥阻塞
redis只有一个线程，线程需要干很多事情，需要干lru,rdb，其他事情，除了IO需要干其他事情
nginx master+worker， worker就是等待连接请求，来了才干事。

IO状态() -----  真正读写
多路复用就是给你返回IO状态。 真正读写都是是同步的

0拷贝kafka

正常一个prodcuer生产一个消息，首先应用程序经过读处理，然后写入到磁盘，来10000，产生10000次读，10000次写，都是通过内核，
优化：mmap，作用于内核空间和用户空间，提供了一种机制，让用户程序直接访问设备内存，相比较于用户空间和内核空间的相互拷贝数据，效率更高。
sendFile: o拷贝，前提：数据不需要加工处理，直接从内核读取到，不用拷贝到用户空间。





